{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#importing all functions in the preprocess.py file \n",
    "#from preprocess import *\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"C:/Users/apt/Desktop/Data/TF/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: Folder Path\n",
    "# Output: Tuple (Label, Indices of the labels, one-hot encoded labels)\n",
    "def get_labels(path=DATA_PATH):\n",
    "    labels = os.listdir(path)\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    return labels, label_indices, to_categorical(label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(path=DATA_PATH):\n",
    "    labels, _, _ = get_labels(path)\n",
    "    data = {}\n",
    "    for label in labels:\n",
    "        data[label] = {}\n",
    "        data[label]['path'] = [path  + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
    "\n",
    "        vectors = []\n",
    "\n",
    "        for wavfile in data[label]['path']:\n",
    "            wave, sr = librosa.load(wavfile, mono=True, sr=None)\n",
    "            # Downsampling\n",
    "            wave = wave[::3]\n",
    "            mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "            vectors.append(mfcc)\n",
    "\n",
    "        data[label]['mfcc'] = vectors\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path=DATA_PATH):\n",
    "    data = prepare_dataset(path)\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for key in data:\n",
    "        for mfcc in data[key]['mfcc']:\n",
    "            dataset.append((key, mfcc))\n",
    "\n",
    "    return dataset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy function to convert wav2mfcc\n",
    "def wav2mfcc(file_path, max_len=11):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    wave = wave[::3]\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "\n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    \n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_array(path=DATA_PATH, max_len=11):\n",
    "    labels, _, _ = get_labels(path)\n",
    "\n",
    "    for label in labels:\n",
    "        # Init mfcc vectors\n",
    "        mfcc_vectors = []\n",
    "\n",
    "        wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
    "        for wavfile in tqdm(wavfiles, \"Saving vectors of label - '{}'\".format(label)):\n",
    "            mfcc = wav2mfcc(wavfile, max_len=max_len)\n",
    "            mfcc_vectors.append(mfcc)\n",
    "        np.save(label + '.npy', mfcc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(split_ratio=0.6, random_state=42):\n",
    "    # Get available labels\n",
    "    labels, indices, _ = get_labels(DATA_PATH)\n",
    "\n",
    "    # Getting first arrays\n",
    "    X = np.load(labels[0] + '.npy')\n",
    "    y = np.zeros(X.shape[0])\n",
    "\n",
    "    # Append all of the dataset into one single array, same goes for y\n",
    "    for i, label in enumerate(labels[1:]):\n",
    "        x = np.load(label + '.npy')\n",
    "        X = np.vstack((X, x))\n",
    "        y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
    "\n",
    "    assert X.shape[0] == len(y)\n",
    "\n",
    "    return train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vectors of label - 'bed': 100%|█████████████████████████████████████████████| 1713/1713 [00:24<00:00, 69.44it/s]\n",
      "Saving vectors of label - 'cat': 100%|█████████████████████████████████████████████| 1733/1733 [00:25<00:00, 68.96it/s]\n",
      "Saving vectors of label - 'happy': 100%|███████████████████████████████████████████| 1742/1742 [00:25<00:00, 68.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is saved in NPY\n"
     ]
    }
   ],
   "source": [
    "# Second dimension of the feature is dim2\n",
    "feature_dim_2 = 11\n",
    "# Save data to array file first\n",
    "save_data_to_array(max_len=feature_dim_2)\n",
    "print(\"The data is saved in NPY\")\n",
    "# # Loading train set and test set\n",
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "\n",
    "# # Feature dimension\n",
    "feature_dim_1 = 20\n",
    "channel = 1\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "verbose = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Reshaping to perform 2D convolution\n",
    "X_train = X_train.reshape(X_train.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_test = X_test.reshape(X_test.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(feature_dim_1, feature_dim_2, channel)))\n",
    "    model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts one sample\n",
    "def predict(filepath, model):\n",
    "    sample = wav2mfcc(filepath)\n",
    "    sample_reshaped = sample.reshape(1, feature_dim_1, feature_dim_2, channel)\n",
    "    return get_labels()[0][np.argmax(model.predict(sample_reshaped))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3112 samples, validate on 2076 samples\n",
      "Epoch 1/10\n",
      "3112/3112 [==============================] - ETA: 30s - loss: 2.9198 - acc: 0.38 - ETA: 18s - loss: 3.4224 - acc: 0.37 - ETA: 13s - loss: 3.4628 - acc: 0.34 - ETA: 11s - loss: 3.5701 - acc: 0.33 - ETA: 9s - loss: 3.2585 - acc: 0.3360 - ETA: 8s - loss: 2.9646 - acc: 0.341 - ETA: 7s - loss: 2.7293 - acc: 0.351 - ETA: 7s - loss: 2.5420 - acc: 0.356 - ETA: 6s - loss: 2.4015 - acc: 0.360 - ETA: 5s - loss: 2.2734 - acc: 0.369 - ETA: 5s - loss: 2.1743 - acc: 0.372 - ETA: 5s - loss: 2.0801 - acc: 0.374 - ETA: 4s - loss: 2.0054 - acc: 0.376 - ETA: 4s - loss: 1.9345 - acc: 0.388 - ETA: 4s - loss: 1.8758 - acc: 0.393 - ETA: 3s - loss: 1.8330 - acc: 0.394 - ETA: 3s - loss: 1.7937 - acc: 0.395 - ETA: 3s - loss: 1.7474 - acc: 0.403 - ETA: 3s - loss: 1.7055 - acc: 0.412 - ETA: 2s - loss: 1.6731 - acc: 0.417 - ETA: 2s - loss: 1.6420 - acc: 0.418 - ETA: 2s - loss: 1.6128 - acc: 0.423 - ETA: 1s - loss: 1.5870 - acc: 0.426 - ETA: 1s - loss: 1.5592 - acc: 0.430 - ETA: 1s - loss: 1.5343 - acc: 0.435 - ETA: 1s - loss: 1.5078 - acc: 0.441 - ETA: 0s - loss: 1.4849 - acc: 0.446 - ETA: 0s - loss: 1.4600 - acc: 0.454 - ETA: 0s - loss: 1.4387 - acc: 0.461 - ETA: 0s - loss: 1.4231 - acc: 0.466 - ETA: 0s - loss: 1.4074 - acc: 0.468 - 9s 3ms/step - loss: 1.4068 - acc: 0.4685 - val_loss: 0.8038 - val_acc: 0.7100\n",
      "Epoch 2/10\n",
      "3112/3112 [==============================] - ETA: 6s - loss: 0.8958 - acc: 0.590 - ETA: 5s - loss: 0.8610 - acc: 0.610 - ETA: 5s - loss: 0.8728 - acc: 0.593 - ETA: 5s - loss: 0.8750 - acc: 0.590 - ETA: 5s - loss: 0.8677 - acc: 0.606 - ETA: 5s - loss: 0.8633 - acc: 0.615 - ETA: 5s - loss: 0.8563 - acc: 0.621 - ETA: 4s - loss: 0.8359 - acc: 0.632 - ETA: 4s - loss: 0.8226 - acc: 0.646 - ETA: 4s - loss: 0.8132 - acc: 0.652 - ETA: 4s - loss: 0.8212 - acc: 0.652 - ETA: 4s - loss: 0.8124 - acc: 0.660 - ETA: 3s - loss: 0.8034 - acc: 0.667 - ETA: 3s - loss: 0.7983 - acc: 0.670 - ETA: 3s - loss: 0.7914 - acc: 0.676 - ETA: 3s - loss: 0.7852 - acc: 0.677 - ETA: 3s - loss: 0.7757 - acc: 0.680 - ETA: 2s - loss: 0.7688 - acc: 0.681 - ETA: 2s - loss: 0.7728 - acc: 0.680 - ETA: 2s - loss: 0.7646 - acc: 0.682 - ETA: 2s - loss: 0.7514 - acc: 0.688 - ETA: 1s - loss: 0.7497 - acc: 0.690 - ETA: 1s - loss: 0.7533 - acc: 0.690 - ETA: 1s - loss: 0.7471 - acc: 0.693 - ETA: 1s - loss: 0.7462 - acc: 0.694 - ETA: 1s - loss: 0.7402 - acc: 0.698 - ETA: 0s - loss: 0.7399 - acc: 0.696 - ETA: 0s - loss: 0.7420 - acc: 0.695 - ETA: 0s - loss: 0.7434 - acc: 0.694 - ETA: 0s - loss: 0.7407 - acc: 0.695 - ETA: 0s - loss: 0.7339 - acc: 0.699 - 8s 2ms/step - loss: 0.7321 - acc: 0.7005 - val_loss: 0.4594 - val_acc: 0.8285\n",
      "Epoch 3/10\n",
      "3112/3112 [==============================] - ETA: 6s - loss: 0.5584 - acc: 0.810 - ETA: 6s - loss: 0.6222 - acc: 0.765 - ETA: 5s - loss: 0.5735 - acc: 0.786 - ETA: 5s - loss: 0.5410 - acc: 0.805 - ETA: 5s - loss: 0.6029 - acc: 0.784 - ETA: 5s - loss: 0.6063 - acc: 0.785 - ETA: 5s - loss: 0.5890 - acc: 0.790 - ETA: 4s - loss: 0.5881 - acc: 0.785 - ETA: 4s - loss: 0.5638 - acc: 0.798 - ETA: 4s - loss: 0.5540 - acc: 0.797 - ETA: 4s - loss: 0.5624 - acc: 0.796 - ETA: 4s - loss: 0.5610 - acc: 0.797 - ETA: 3s - loss: 0.5504 - acc: 0.800 - ETA: 3s - loss: 0.5505 - acc: 0.800 - ETA: 3s - loss: 0.5470 - acc: 0.802 - ETA: 3s - loss: 0.5459 - acc: 0.803 - ETA: 3s - loss: 0.5436 - acc: 0.804 - ETA: 2s - loss: 0.5409 - acc: 0.803 - ETA: 2s - loss: 0.5382 - acc: 0.803 - ETA: 2s - loss: 0.5328 - acc: 0.807 - ETA: 2s - loss: 0.5267 - acc: 0.809 - ETA: 1s - loss: 0.5237 - acc: 0.811 - ETA: 1s - loss: 0.5273 - acc: 0.810 - ETA: 1s - loss: 0.5339 - acc: 0.809 - ETA: 1s - loss: 0.5375 - acc: 0.807 - ETA: 1s - loss: 0.5383 - acc: 0.808 - ETA: 0s - loss: 0.5351 - acc: 0.810 - ETA: 0s - loss: 0.5308 - acc: 0.812 - ETA: 0s - loss: 0.5301 - acc: 0.811 - ETA: 0s - loss: 0.5342 - acc: 0.808 - ETA: 0s - loss: 0.5347 - acc: 0.808 - 8s 2ms/step - loss: 0.5340 - acc: 0.8091 - val_loss: 0.3683 - val_acc: 0.8632\n",
      "Epoch 4/10\n",
      "3112/3112 [==============================] - ETA: 5s - loss: 0.5435 - acc: 0.820 - ETA: 5s - loss: 0.4847 - acc: 0.830 - ETA: 5s - loss: 0.5102 - acc: 0.816 - ETA: 5s - loss: 0.4936 - acc: 0.825 - ETA: 5s - loss: 0.5122 - acc: 0.812 - ETA: 4s - loss: 0.5168 - acc: 0.813 - ETA: 4s - loss: 0.5160 - acc: 0.814 - ETA: 4s - loss: 0.5110 - acc: 0.813 - ETA: 4s - loss: 0.4941 - acc: 0.815 - ETA: 4s - loss: 0.4803 - acc: 0.821 - ETA: 3s - loss: 0.4754 - acc: 0.822 - ETA: 3s - loss: 0.4826 - acc: 0.822 - ETA: 3s - loss: 0.4738 - acc: 0.828 - ETA: 3s - loss: 0.4689 - acc: 0.827 - ETA: 3s - loss: 0.4602 - acc: 0.831 - ETA: 2s - loss: 0.4532 - acc: 0.833 - ETA: 2s - loss: 0.4567 - acc: 0.831 - ETA: 2s - loss: 0.4692 - acc: 0.825 - ETA: 2s - loss: 0.4672 - acc: 0.825 - ETA: 2s - loss: 0.4621 - acc: 0.828 - ETA: 2s - loss: 0.4589 - acc: 0.829 - ETA: 1s - loss: 0.4536 - acc: 0.832 - ETA: 1s - loss: 0.4550 - acc: 0.833 - ETA: 1s - loss: 0.4503 - acc: 0.834 - ETA: 1s - loss: 0.4591 - acc: 0.834 - ETA: 1s - loss: 0.4598 - acc: 0.833 - ETA: 0s - loss: 0.4592 - acc: 0.833 - ETA: 0s - loss: 0.4581 - acc: 0.834 - ETA: 0s - loss: 0.4557 - acc: 0.836 - ETA: 0s - loss: 0.4503 - acc: 0.838 - ETA: 0s - loss: 0.4510 - acc: 0.838 - 7s 2ms/step - loss: 0.4501 - acc: 0.8384 - val_loss: 0.2891 - val_acc: 0.9003\n",
      "Epoch 5/10\n",
      "3112/3112 [==============================] - ETA: 6s - loss: 0.2538 - acc: 0.900 - ETA: 5s - loss: 0.2845 - acc: 0.890 - ETA: 5s - loss: 0.2972 - acc: 0.893 - ETA: 5s - loss: 0.3226 - acc: 0.885 - ETA: 5s - loss: 0.3174 - acc: 0.888 - ETA: 5s - loss: 0.3341 - acc: 0.870 - ETA: 5s - loss: 0.3322 - acc: 0.871 - ETA: 4s - loss: 0.3304 - acc: 0.873 - ETA: 4s - loss: 0.3351 - acc: 0.874 - ETA: 4s - loss: 0.3483 - acc: 0.874 - ETA: 4s - loss: 0.3412 - acc: 0.878 - ETA: 4s - loss: 0.3346 - acc: 0.880 - ETA: 3s - loss: 0.3381 - acc: 0.880 - ETA: 3s - loss: 0.3347 - acc: 0.880 - ETA: 3s - loss: 0.3362 - acc: 0.880 - ETA: 3s - loss: 0.3408 - acc: 0.878 - ETA: 2s - loss: 0.3399 - acc: 0.877 - ETA: 2s - loss: 0.3344 - acc: 0.880 - ETA: 2s - loss: 0.3296 - acc: 0.881 - ETA: 2s - loss: 0.3238 - acc: 0.884 - ETA: 2s - loss: 0.3223 - acc: 0.885 - ETA: 1s - loss: 0.3262 - acc: 0.884 - ETA: 1s - loss: 0.3347 - acc: 0.880 - ETA: 1s - loss: 0.3435 - acc: 0.876 - ETA: 1s - loss: 0.3428 - acc: 0.875 - ETA: 1s - loss: 0.3439 - acc: 0.875 - ETA: 0s - loss: 0.3447 - acc: 0.876 - ETA: 0s - loss: 0.3393 - acc: 0.880 - ETA: 0s - loss: 0.3390 - acc: 0.880 - ETA: 0s - loss: 0.3340 - acc: 0.883 - ETA: 0s - loss: 0.3332 - acc: 0.882 - 8s 2ms/step - loss: 0.3332 - acc: 0.8824 - val_loss: 0.3051 - val_acc: 0.8931\n",
      "Epoch 6/10\n",
      "3112/3112 [==============================] - ETA: 5s - loss: 0.2382 - acc: 0.910 - ETA: 5s - loss: 0.2203 - acc: 0.930 - ETA: 5s - loss: 0.2486 - acc: 0.920 - ETA: 5s - loss: 0.2870 - acc: 0.902 - ETA: 5s - loss: 0.2964 - acc: 0.890 - ETA: 5s - loss: 0.2974 - acc: 0.890 - ETA: 4s - loss: 0.2903 - acc: 0.891 - ETA: 4s - loss: 0.2900 - acc: 0.895 - ETA: 4s - loss: 0.2885 - acc: 0.894 - ETA: 4s - loss: 0.2882 - acc: 0.893 - ETA: 4s - loss: 0.2876 - acc: 0.897 - ETA: 3s - loss: 0.2838 - acc: 0.898 - ETA: 3s - loss: 0.2784 - acc: 0.899 - ETA: 3s - loss: 0.2783 - acc: 0.898 - ETA: 3s - loss: 0.2798 - acc: 0.899 - ETA: 3s - loss: 0.2750 - acc: 0.902 - ETA: 2s - loss: 0.2752 - acc: 0.902 - ETA: 2s - loss: 0.2744 - acc: 0.900 - ETA: 2s - loss: 0.2733 - acc: 0.901 - ETA: 2s - loss: 0.2757 - acc: 0.901 - ETA: 2s - loss: 0.2778 - acc: 0.902 - ETA: 1s - loss: 0.2745 - acc: 0.903 - ETA: 1s - loss: 0.2774 - acc: 0.902 - ETA: 1s - loss: 0.2803 - acc: 0.901 - ETA: 1s - loss: 0.2930 - acc: 0.897 - ETA: 1s - loss: 0.2933 - acc: 0.896 - ETA: 0s - loss: 0.2925 - acc: 0.897 - ETA: 0s - loss: 0.2911 - acc: 0.898 - ETA: 0s - loss: 0.2902 - acc: 0.899 - ETA: 0s - loss: 0.2914 - acc: 0.897 - ETA: 0s - loss: 0.2887 - acc: 0.899 - 8s 2ms/step - loss: 0.2888 - acc: 0.8991 - val_loss: 0.2944 - val_acc: 0.8858\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3112/3112 [==============================] - ETA: 5s - loss: 0.4024 - acc: 0.850 - ETA: 5s - loss: 0.3763 - acc: 0.860 - ETA: 5s - loss: 0.3458 - acc: 0.866 - ETA: 5s - loss: 0.3268 - acc: 0.882 - ETA: 5s - loss: 0.3061 - acc: 0.892 - ETA: 5s - loss: 0.2847 - acc: 0.898 - ETA: 4s - loss: 0.2663 - acc: 0.902 - ETA: 4s - loss: 0.2543 - acc: 0.907 - ETA: 4s - loss: 0.2518 - acc: 0.907 - ETA: 4s - loss: 0.2635 - acc: 0.905 - ETA: 4s - loss: 0.2604 - acc: 0.906 - ETA: 3s - loss: 0.2560 - acc: 0.909 - ETA: 3s - loss: 0.2588 - acc: 0.910 - ETA: 3s - loss: 0.2527 - acc: 0.912 - ETA: 3s - loss: 0.2544 - acc: 0.911 - ETA: 3s - loss: 0.2489 - acc: 0.914 - ETA: 2s - loss: 0.2518 - acc: 0.912 - ETA: 2s - loss: 0.2527 - acc: 0.912 - ETA: 2s - loss: 0.2496 - acc: 0.912 - ETA: 2s - loss: 0.2471 - acc: 0.913 - ETA: 2s - loss: 0.2468 - acc: 0.913 - ETA: 1s - loss: 0.2525 - acc: 0.911 - ETA: 1s - loss: 0.2590 - acc: 0.909 - ETA: 1s - loss: 0.2608 - acc: 0.907 - ETA: 1s - loss: 0.2580 - acc: 0.908 - ETA: 1s - loss: 0.2616 - acc: 0.907 - ETA: 0s - loss: 0.2616 - acc: 0.908 - ETA: 0s - loss: 0.2585 - acc: 0.909 - ETA: 0s - loss: 0.2587 - acc: 0.908 - ETA: 0s - loss: 0.2588 - acc: 0.908 - ETA: 0s - loss: 0.2571 - acc: 0.909 - 8s 2ms/step - loss: 0.2568 - acc: 0.9097 - val_loss: 0.2984 - val_acc: 0.8931\n",
      "Epoch 8/10\n",
      "3112/3112 [==============================] - ETA: 6s - loss: 0.3749 - acc: 0.880 - ETA: 6s - loss: 0.3809 - acc: 0.880 - ETA: 5s - loss: 0.3508 - acc: 0.886 - ETA: 5s - loss: 0.3287 - acc: 0.892 - ETA: 5s - loss: 0.3122 - acc: 0.894 - ETA: 5s - loss: 0.3014 - acc: 0.901 - ETA: 5s - loss: 0.2794 - acc: 0.907 - ETA: 5s - loss: 0.2677 - acc: 0.911 - ETA: 4s - loss: 0.2567 - acc: 0.918 - ETA: 4s - loss: 0.2496 - acc: 0.920 - ETA: 4s - loss: 0.2426 - acc: 0.919 - ETA: 4s - loss: 0.2468 - acc: 0.915 - ETA: 4s - loss: 0.2422 - acc: 0.916 - ETA: 3s - loss: 0.2369 - acc: 0.918 - ETA: 3s - loss: 0.2342 - acc: 0.920 - ETA: 3s - loss: 0.2403 - acc: 0.917 - ETA: 3s - loss: 0.2397 - acc: 0.917 - ETA: 2s - loss: 0.2370 - acc: 0.915 - ETA: 2s - loss: 0.2389 - acc: 0.913 - ETA: 2s - loss: 0.2354 - acc: 0.914 - ETA: 2s - loss: 0.2354 - acc: 0.914 - ETA: 2s - loss: 0.2306 - acc: 0.916 - ETA: 1s - loss: 0.2283 - acc: 0.917 - ETA: 1s - loss: 0.2230 - acc: 0.919 - ETA: 1s - loss: 0.2189 - acc: 0.920 - ETA: 1s - loss: 0.2165 - acc: 0.921 - ETA: 0s - loss: 0.2146 - acc: 0.922 - ETA: 0s - loss: 0.2131 - acc: 0.922 - ETA: 0s - loss: 0.2141 - acc: 0.922 - ETA: 0s - loss: 0.2132 - acc: 0.922 - ETA: 0s - loss: 0.2140 - acc: 0.921 - 8s 3ms/step - loss: 0.2133 - acc: 0.9219 - val_loss: 0.2159 - val_acc: 0.9297\n",
      "Epoch 9/10\n",
      "3112/3112 [==============================] - ETA: 5s - loss: 0.1753 - acc: 0.920 - ETA: 5s - loss: 0.1467 - acc: 0.940 - ETA: 5s - loss: 0.1917 - acc: 0.933 - ETA: 5s - loss: 0.1924 - acc: 0.932 - ETA: 5s - loss: 0.1711 - acc: 0.940 - ETA: 5s - loss: 0.1678 - acc: 0.945 - ETA: 4s - loss: 0.1615 - acc: 0.947 - ETA: 4s - loss: 0.1649 - acc: 0.945 - ETA: 4s - loss: 0.1603 - acc: 0.944 - ETA: 4s - loss: 0.1579 - acc: 0.945 - ETA: 4s - loss: 0.1606 - acc: 0.944 - ETA: 3s - loss: 0.1743 - acc: 0.940 - ETA: 3s - loss: 0.1742 - acc: 0.940 - ETA: 3s - loss: 0.1718 - acc: 0.941 - ETA: 3s - loss: 0.1733 - acc: 0.940 - ETA: 3s - loss: 0.1712 - acc: 0.940 - ETA: 2s - loss: 0.1717 - acc: 0.940 - ETA: 2s - loss: 0.1738 - acc: 0.938 - ETA: 2s - loss: 0.1731 - acc: 0.940 - ETA: 2s - loss: 0.1825 - acc: 0.939 - ETA: 2s - loss: 0.1842 - acc: 0.938 - ETA: 1s - loss: 0.1858 - acc: 0.937 - ETA: 1s - loss: 0.1860 - acc: 0.937 - ETA: 1s - loss: 0.1823 - acc: 0.938 - ETA: 1s - loss: 0.1786 - acc: 0.939 - ETA: 1s - loss: 0.1792 - acc: 0.938 - ETA: 0s - loss: 0.1760 - acc: 0.940 - ETA: 0s - loss: 0.1797 - acc: 0.938 - ETA: 0s - loss: 0.1756 - acc: 0.941 - ETA: 0s - loss: 0.1774 - acc: 0.941 - ETA: 0s - loss: 0.1811 - acc: 0.939 - 8s 2ms/step - loss: 0.1820 - acc: 0.9393 - val_loss: 0.2000 - val_acc: 0.9335\n",
      "Epoch 10/10\n",
      "3112/3112 [==============================] - ETA: 5s - loss: 0.1251 - acc: 0.980 - ETA: 5s - loss: 0.1373 - acc: 0.960 - ETA: 5s - loss: 0.1228 - acc: 0.960 - ETA: 5s - loss: 0.1285 - acc: 0.957 - ETA: 5s - loss: 0.1252 - acc: 0.960 - ETA: 5s - loss: 0.1295 - acc: 0.956 - ETA: 5s - loss: 0.1443 - acc: 0.952 - ETA: 4s - loss: 0.1427 - acc: 0.953 - ETA: 4s - loss: 0.1516 - acc: 0.951 - ETA: 4s - loss: 0.1565 - acc: 0.949 - ETA: 4s - loss: 0.1569 - acc: 0.948 - ETA: 4s - loss: 0.1633 - acc: 0.945 - ETA: 3s - loss: 0.1622 - acc: 0.944 - ETA: 3s - loss: 0.1662 - acc: 0.942 - ETA: 3s - loss: 0.1644 - acc: 0.944 - ETA: 3s - loss: 0.1673 - acc: 0.941 - ETA: 2s - loss: 0.1680 - acc: 0.941 - ETA: 2s - loss: 0.1638 - acc: 0.943 - ETA: 2s - loss: 0.1637 - acc: 0.943 - ETA: 2s - loss: 0.1634 - acc: 0.943 - ETA: 2s - loss: 0.1630 - acc: 0.943 - ETA: 1s - loss: 0.1612 - acc: 0.944 - ETA: 1s - loss: 0.1579 - acc: 0.945 - ETA: 1s - loss: 0.1606 - acc: 0.943 - ETA: 1s - loss: 0.1593 - acc: 0.944 - ETA: 1s - loss: 0.1583 - acc: 0.945 - ETA: 0s - loss: 0.1585 - acc: 0.945 - ETA: 0s - loss: 0.1572 - acc: 0.945 - ETA: 0s - loss: 0.1562 - acc: 0.946 - ETA: 0s - loss: 0.1550 - acc: 0.946 - ETA: 0s - loss: 0.1570 - acc: 0.946 - 8s 2ms/step - loss: 0.1571 - acc: 0.9460 - val_loss: 0.3190 - val_acc: 0.8902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21982752eb8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.fit(X_train, y_train_hot, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(X_test, y_test_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(predict('C:/Users/apt/Desktop/Data/TF/cat/0e5193e6_nohash_0.wav', model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
